{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Settings\n",
    "\n",
    "Import Libraries that are used in the Reinforcement Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import copy\n",
    "from itertools import count\n",
    "from hopperenv import *     # Our Rocket Hopper Environment\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy\n",
    "import gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.1+cpu\n",
      "NumPy version: 1.26.2\n",
      "Gym version: 0.26.2\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Gym version: {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for plotting\n",
    "def plot(states):\n",
    "    num_states = states.shape[0]\n",
    "    time = np.arange(states.shape[1])\n",
    "\n",
    "    plt.figure(figsize=(10, 2*num_states))\n",
    "\n",
    "    names = [\"position [m]\",\"velocity [m/s]\",\"acceleration [m/s²]\",\"p_set [bar]\",\"p_actual [bar]\",\"x_target [m]\",\"error [m]\",\"reward\",\"?\"]\n",
    "    colors = [\"tab:blue\",\"tab:orange\",\"tab:purple\",\"tab:green\",\"tab:green\",\"tab:grey\",\"tab:red\",\"tab:cyan\",\"black\"]\n",
    "\n",
    "    for i in range(num_states):\n",
    "        plt.subplot(num_states, 1, i + 1)\n",
    "        plt.plot(time, states[i, :], label=names[i],color=colors[i])\n",
    "        if i == 6: # error\n",
    "            plt.fill_between(time, 0, states[i, :], color=colors[i], alpha=0.5)\n",
    "        plt.ylabel(names[i])\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.xlabel('Time [steps]')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Settings\n",
    "\n",
    "The configuration initially follows the [supplementary information section](https://arxiv.org/pdf/1509.02971.pdf?ref=blog.paperspace.com) of the DDPG paper in page 11.\n",
    "\n",
    "Hyperparameters not only affect the reinforcement learning algorithm but also th the exploration strategy and the environment. DDPG generally provides good results in continuous control problems (Duan et al., 2016) but is more sensitive to numerous hyperparameters than other algorithms (Rupam Mahmood et al., 2018).\n",
    "\n",
    "These are grouped into DDPG, Exploration, and Environmental Hyperparameters as outlined below:\n",
    "\n",
    "#### DDPG Hyperparameters\n",
    "\n",
    "- **Batch Size**: Number of samples used during an update (gradient descent)\n",
    "- **Gamma**: Discount factor ∈ [0, 1] defines up to what extent future rewards influence the return in time step t.\n",
    "- **Actor and Critic Learning Rates**: Defines the step size in solution direction and controls how strongly the weights of the artificial neural network are updated by the loss gradient.\n",
    "- **Number of Neurons**: Layer Size of the neural network.\n",
    "- **Regularization Factor Critic**: Method to prevent overfitting and improve models generalization properties.\n",
    "- **Memory Capacity**: Size of the memory containing the batch samples.\n",
    "\n",
    "\n",
    "#### Exploration Hyperparameters\n",
    "\n",
    "Sometimes exploration is not necessary - it depends on the environment. If uncertain, an exploration strategy is applied - an Ornstein-Uhlenbeck Process (OUP) is selected in this case.\n",
    "\n",
    "The hyperparameters of the OUP are:\n",
    "\n",
    "- **Mean**: Mean value of the OUP\n",
    "- **Theta**: Reversion rate of the OUP\n",
    "- **Sigma**: Standard deviation of the OUP\n",
    "\n",
    "#### Environmental Hyperparameters\n",
    "\n",
    "The environment has context-specific hyperparameters. Depending on the environment, arbitrary ones can be included. For example. the training duration can be one. A longer training duration allows the agent to interact with his environment for a longer  period of time at the expense of computational cost.\n",
    "\n",
    "\n",
    "### Hyperparameter Optimization Methods\n",
    "\n",
    "The suitable hyperparameters are problem-specific and the optimal hyperparameter combination is often not intuitive. The widespread manual choice of hyperparameters therefore requires experties and is time-consuming (Liessner et.al., 2019).\n",
    "\n",
    "Several strategies are presented that can be followed depending on the approach. There are **Model-free Approaches** and **Model-based Approaches**.\n",
    "\n",
    "- **Model-free Approaches**:\n",
    "    - Grid Search - discrete, grid-shaped substed instead of the entire parameter space.\n",
    "    - Random Search - selects hyperparameters from the equally distributed search space.\n",
    "    - These algorithms can use the knowledge gained during their processing to adapt and intensify the search in areasof the search space with high *result potential*.\n",
    "\n",
    "Bergstra and Bengio proved that the random choice of hyperparameter combinations is more efficient than searching a grid subset (Bergstra and Bengio, 2012). These approaches prevent the convergence into local optima but are extremely time-consuming  due to the *curse of dimensionality* in large  parameter spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE=1000000 # Buffer size of 1 million entries\n",
    "\n",
    "BATCH_SIZE=128   # Sampling from memory - This can be 128 for more complex tasks such as Hopper\n",
    "UPDATE_ITERATION=100 # Number of iterations in the replay buffer\n",
    "\n",
    "tau=0.01       # Target Network HyperParameters (soft updating)\n",
    "gamma=0.99      # ?\n",
    "\n",
    "directory = './'\n",
    "\n",
    "# Neural Network architecture:\n",
    "H1=64  # Neuron of 1st Layers #400 #20 # 64\n",
    "H2=H1*2  # Neurons of 2nd layers #300 #64 # 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration \n",
    "\n",
    "\n",
    "#### Exploration vs Exploitation\n",
    "\n",
    "A major challenge in reinforcement learning is the balance between exploration and exploitation. In order to achieve high rewards, the agent has to choose actions that have proven to be particularlu rewarding in the past. In order to discover such actions in the first place, new actions have to be tested. This means the agent has to *exploit* knowledge already learned from the environment to get a reward, and at the same time explore other actions to have a better strategy in the future (Sutton and Barto, 1998). \n",
    "\n",
    "Various exploration strategies are available for this purpose. In (Plappert et al., 2017), M. Plappert compares several exploration strategies for continuous action spaces.  Additionally Lilicrap compares the uncorrelated action noise to the additive Gaussian action noise (based on the OUP). The latter changes less abruptly from one timestep to the next (Lilicrap et al., 2015). This characteristic can be beneficial for the control of physical actuators.\n",
    "\n",
    "\n",
    "To ensure **exploration**, we have to add noise to the action. An **Ornstein-Uhlenbeck** process is chosen to add noise in a smooth way, suitable for continuous control tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OU_Noise(object):\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\n",
    "    \n",
    "    The OU_Noise class has four attributes\n",
    "    \n",
    "        size: the size of the noise vector to be generated\n",
    "        mu: the mean of the noise, set to 0 by default\n",
    "        theta: the rate of mean reversion, controlling how quickly the noise returns to the mean\n",
    "        sigma: the volatility of the noise, controlling the magnitude of fluctuations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.25):\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\n",
    "        This method uses the current state of the noise and generates the next sample\n",
    "        \"\"\"\n",
    "        dx = self.theta * (self.mu - self.state) + self.sigma * np.array([np.random.normal() for _ in range(len(self.state))])\n",
    "        self.state += dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer\n",
    "Implement a simple replay buffer without priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay_buffer():\n",
    "    '''\n",
    "    Code based on:\n",
    "    https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "    Expects tuples of (state, next_state, action, reward, done)\n",
    "    '''\n",
    "    def __init__(self, max_size=BUFFER_SIZE):\n",
    "        \"\"\"Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        \"\"\"\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def push(self, data):\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = data\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(data)\n",
    "\n",
    "    def sample(self, BATCH_SIZE):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        BATCH_SIZE: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        state: np.array\n",
    "            batch of state or observations\n",
    "        action: np.array\n",
    "            batch of actions executed given a state\n",
    "        reward: np.array\n",
    "            rewards received as results of executing action\n",
    "        next_state: np.array\n",
    "            next state next state or observations seen after executing action\n",
    "        done: np.array\n",
    "            done[i] = 1 if executing ation[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        ind = np.random.randint(0, len(self.storage), size=BATCH_SIZE)\n",
    "        state, next_state, action, reward, done = [], [], [], [], []\n",
    "\n",
    "        for i in ind:\n",
    "            st, n_st, act, rew, dn = self.storage[i]\n",
    "            state.append(np.array(st, copy=False))\n",
    "            next_state.append(np.array(n_st, copy=False))\n",
    "            action.append(np.array(act, copy=False))\n",
    "            reward.append(np.array(rew, copy=False))\n",
    "            done.append(np.array(dn, copy=False))\n",
    "\n",
    "        return np.array(state), np.array(next_state), np.array(action), np.array(reward).reshape(-1, 1), np.array(done).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture\n",
    "\n",
    "Network architecture is defined:\n",
    "\n",
    "- **Actor** is composed of:\n",
    "    - 3 FC (Fully-connected) Layers \n",
    "    - Hyperbolic tangent as the activation function -> Deals with a *-1,1* value range\n",
    "\n",
    "- **Critic** is composed of:\n",
    "    - Input: *state* & *action*\n",
    "    - Output: Q-value after 3 FC Layers\n",
    "\n",
    "\n",
    "In complex tasks, a *higher batch size* is required (128 instead of 64) and *batch normalisation layers* between input and hidden layers in both actor and critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"\n",
    "    The Actor model takes in a state observation as input and \n",
    "    outputs an action, which is a continuous value.\n",
    "    \n",
    "    It consists of four fully coonected linear layers with ReLU activation functions and \n",
    "    a final output layer selects one single optimized action for the state\n",
    "    \"\"\"\n",
    "    def __init__(self, n_states, action_dim, H1):\n",
    "        super(Actor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_states, H1), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(H1, H1), \n",
    "            nn.ReLU(), \n",
    "            # nn.Linear(H1, H1), \n",
    "            # nn.ReLU(), \n",
    "            nn.Linear(H1, action_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"\n",
    "    The Critic model takes in both a state observation and an action as input and \n",
    "    outputs a Q-value, which estimates the expected total reward for the current state-action pair. \n",
    "    \n",
    "    It consists of four linear layers with ReLU activation functions, \n",
    "    State and action inputs are concatenated before being fed into the first linear layer. \n",
    "    \n",
    "    The output layer has a single output, representing the Q-value\n",
    "    \"\"\"\n",
    "    def __init__(self, n_states, action_dim, H2):\n",
    "        super(Critic, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_states + action_dim, H2), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(H2, H2), \n",
    "            nn.ReLU(), \n",
    "            # nn.Linear(H2, H2), \n",
    "            # nn.ReLU(), \n",
    "            nn.Linear(H2, action_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        return self.net(torch.cat((state, action), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Setup \n",
    "\n",
    "GPU is used for faster training if your machine has one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job will run on cpu\n"
     ]
    }
   ],
   "source": [
    "#set GPU for faster training\n",
    "cuda = torch.cuda.is_available() #check for CUDA\n",
    "device   = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(\"Job will run on {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPG Agent\n",
    "\n",
    "Creates a replay buffer, an actor-critic network and their corresponding target networks. Additionally, it also initializes the optimizer for both networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG(object):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        \"\"\"\n",
    "        Initializes the DDPG agent. \n",
    "        Takes three arguments:\n",
    "               state_dim which is the dimensionality of the state space, \n",
    "               action_dim which is the dimensionality of the action space, and \n",
    "               max_action which is the maximum value an action can take. \n",
    "        \n",
    "        Creates a replay buffer, an actor-critic  networks and their corresponding target networks. \n",
    "        It also initializes the optimizer for both actor and critic networks alog with \n",
    "        counters to track the number of training iterations.\n",
    "        \"\"\"\n",
    "        self.replay_buffer = Replay_buffer()\n",
    "        \n",
    "        self.actor = Actor(state_dim, action_dim, H1).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim,  H1).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-2)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim,  H2).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim,  H2).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=5e-2)\n",
    "        # learning rate\n",
    "\n",
    "        \n",
    "\n",
    "        self.num_critic_update_iteration = 0\n",
    "        self.num_actor_update_iteration = 0\n",
    "        self.num_training = 0\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        takes the current state as input and returns an action to take in that state. \n",
    "        It uses the actor network to map the state to an action.\n",
    "        \"\"\"\n",
    "        #print(f\"Doing an Action: state {state} reshaped {state.reshape(1, -1)}\")\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        # print(state) # tensor([[0., 0.]])\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        updates the actor and critic networks using a batch of samples from the replay buffer. \n",
    "        For each sample in the batch, it computes the target Q value using the target critic network and the target actor network. \n",
    "        It then computes the current Q value using the critic network and the action taken by the actor network. \n",
    "        \n",
    "        It computes the critic loss as the mean squared error between the target Q value and the current Q value, and \n",
    "        updates the critic network using gradient descent. \n",
    "        \n",
    "        It then computes the actor loss as the negative mean Q value using the critic network and the actor network, and \n",
    "        updates the actor network using gradient ascent. \n",
    "        \n",
    "        Finally, it updates the target networks using soft updates, where a small fraction of the actor and critic network weights \n",
    "        are transferred to their target counterparts. \n",
    "        This process is repeated for a fixed number of iterations.\n",
    "        \"\"\"\n",
    "\n",
    "        for it in range(UPDATE_ITERATION):\n",
    "            # For each Sample in replay buffer batch\n",
    "            state, next_state, action, reward, done = self.replay_buffer.sample(BATCH_SIZE)\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            #print(state)\n",
    "            action = torch.FloatTensor(action).to(device)\n",
    "            next_state = torch.FloatTensor(next_state).to(device)\n",
    "            done = torch.FloatTensor(1-done).to(device)\n",
    "            reward = torch.FloatTensor(reward).to(device)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            target_Q = self.critic_target(next_state, self.actor_target(next_state))\n",
    "            target_Q = reward + (done * gamma * target_Q).detach()\n",
    "\n",
    "            # Get current Q estimate\n",
    "            current_Q = self.critic(state, action)\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = F.mse_loss(current_Q, target_Q)\n",
    "            \n",
    "            # Optimize the critic\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # Compute actor loss as the negative mean Q value using the critic network and the actor network\n",
    "            actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "            \n",
    "\n",
    "            # Optimize the actor\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Update the frozen target models using \n",
    "            soft updates, where \n",
    "            tau,a small fraction of the actor and critic network weights are transferred to their target counterparts. \n",
    "            \"\"\"\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            \n",
    "           \n",
    "            self.num_actor_update_iteration += 1\n",
    "            self.num_critic_update_iteration += 1\n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        Saves the state dictionaries of the actor and critic networks to files\n",
    "        \"\"\"\n",
    "        torch.save(self.actor.state_dict(), directory + 'actor.pth')\n",
    "        torch.save(self.critic.state_dict(), directory + 'critic.pth')\n",
    "        \n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Loads the state dictionaries of the actor and critic networks to files\n",
    "        \"\"\"\n",
    "        self.actor.load_state_dict(torch.load(directory + 'actor.pth'))\n",
    "        self.critic.load_state_dict(torch.load(directory + 'critic.pth'))\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a DDPG Instance\n",
    "\n",
    "Call the environment to be trained. Always use the same seed for data reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Example environment:\n",
    "# create the environment\n",
    "# env_name='MountainCarContinuous-v0'\n",
    "# env = gym.make(env_name)\n",
    "\n",
    "# new environment\n",
    "env = HopperEnv()\n",
    "\n",
    "\n",
    "# XX\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define different parameters for training the agent:\n",
    "    # Episodes:\n",
    "MAX_EPISODE=100          # Number of episodes 200\n",
    "ep_r = 0          # Initial episode reward: normally 0 or -infinity\n",
    "    # Steps:\n",
    "MAX_TIME_STEPS=300      # Number of steps taken per episode before moving on # 5000\n",
    "total_step = 0          # Initialize step count in each episode: [0, MAX_TIME_STEPS]\n",
    "\n",
    "# # To render the environmnet:\n",
    "# render=True\n",
    "# render_interval=10\n",
    "\n",
    "# For Reproducibility:\n",
    "# env.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Environment action and states\n",
    "state_dim = env.observation_space.shape[0]\n",
    "print(state_dim)\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "min_action = float(env.action_space.low[0])\n",
    "min_Val = torch.tensor(1e-7).float().to(device) \n",
    "\n",
    "\n",
    "# Exploration Noise\n",
    "exploration_noise=0.5 # [bar]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate Through Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map(value, from_min, from_max, to_min, to_max):\n",
    "    return np.clip((value - from_min) / (from_max - from_min) * (to_max - to_min) + to_min, to_min, to_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dim: 3, Action dim: 1\n",
      "Episode: 99 | Total Reward: 87.37 | Simulation 156.00 ms | Agend update 688.00 ms  \r"
     ]
    }
   ],
   "source": [
    "# Create a DDPG instance\n",
    "agent = DDPG(state_dim, action_dim)\n",
    "print(\"State dim: {}, Action dim: {}\".format(state_dim, action_dim))\n",
    "\n",
    "logging = False\n",
    "\n",
    "episodes = []\n",
    "best_episode = {'index':None,'value':0}\n",
    "score_hist = [] # Initialize the list where all historical rewards of each episode are stored\n",
    "\n",
    "# Train the agent for the number of episodes set:\n",
    "\n",
    "for i in range(MAX_EPISODE):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "    state = env.reset() # [x_target, x, a]\n",
    "    log = np.zeros((8,MAX_TIME_STEPS))\n",
    "    \n",
    "    # state=state[0]\n",
    "\n",
    "    start_time = time.time()\n",
    "    for  t in range(MAX_TIME_STEPS):\n",
    "\n",
    "        action = agent.select_action(state) # range ? [-1..1] (?)\n",
    "        action = map(action, -1, 1, min_action, max_action) # [0..7]\n",
    "        \n",
    "        #print(action)\n",
    "        # Add Gaussian noise to actions for exploration\n",
    "        action = (action + np.random.normal(0, exploration_noise, size=action_dim)).clip(min_action, max_action)\n",
    "        # print(env.step(action))\n",
    "        #action += OU_Noise.sample(action)\n",
    "        #action = np.array([7]) # test full throttle\n",
    "\n",
    "        y, reward, done, info = env.step(action)\n",
    "        \n",
    "        if logging:\n",
    "            \n",
    "            log[0,t] = y[1] # x\n",
    "            log[1,t] = info[1] # v\n",
    "            log[2,t] = y[2] # a\n",
    "            log[3,t] = action[0] # p_set\n",
    "            log[4,t] = info[0] # p_actual\n",
    "            log[5,t] = y[0] # x_target\n",
    "            log[6,t] = y[1] - y[0] # error\n",
    "            log[7,t] = reward # reward\n",
    "        \n",
    "\n",
    "        total_reward += reward\n",
    "        # if render and i >= render_interval : env.render()\n",
    "        agent.replay_buffer.push((state, y, action, reward, float(done)))\n",
    "\n",
    "        state = y\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        step += 1\n",
    "\n",
    "\n",
    "    if total_reward > best_episode['value']:\n",
    "        best_episode['value'] = total_reward\n",
    "        best_episode['index'] = i\n",
    "    \n",
    "    score_hist.append(total_reward)\n",
    "    if logging:\n",
    "        episodes.append(log)\n",
    "    \n",
    "    total_step += step+1\n",
    "\n",
    "    sim_time = time.time()\n",
    "    \n",
    "    agent.update()\n",
    "    if i % 100 == 0:\n",
    "        agent.save()\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Episode: {i} | Total Reward: {total_reward:5.2f} | Simulation {1000*(sim_time-start_time):4.2f} ms | Agend update {1000*(end_time-start_time):4.2f} ms \", end='\\r', flush=True)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 47 achieved Reward 141.95\n"
     ]
    }
   ],
   "source": [
    "def analyse(j):\n",
    "    print(f\"Episode {j} achieved Reward {score_hist[j]:5.2f}\")\n",
    "    if logging:\n",
    "        plot(episodes[j])\n",
    "\n",
    "# Plot a random episode \n",
    "#analyse(random.randint(0, len(episodes)))\n",
    "\n",
    "# Plot best episode\n",
    "analyse(best_episode['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "def plot_learning_curve(x, scores):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Running average of previous 100 scores')\n",
    "    #plt.savefig(figure_file)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Reward History Average\n",
    "\n",
    "Over all the episodes, the reward history average is plotted per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [i+1 for i in range(len(score_hist))]\n",
    "plot_learning_curve(x, score_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode \t0, the episode reward is \t89.66\n",
      "Episode \t1, the episode reward is \t29.79\n",
      "Episode \t2, the episode reward is \t52.73\n",
      "Episode \t3, the episode reward is \t31.03\n",
      "Episode \t4, the episode reward is \t30.90\n",
      "Episode \t5, the episode reward is \t162.05\n",
      "Episode \t6, the episode reward is \t39.51\n",
      "Episode \t7, the episode reward is \t88.24\n",
      "Episode \t8, the episode reward is \t110.31\n",
      "Episode \t9, the episode reward is \t32.97\n"
     ]
    }
   ],
   "source": [
    "test_iteration=10              # 100\n",
    "max_length_of_trajectory=100   # 1000 \n",
    "for i in range(test_iteration):\n",
    "    state = env.reset()\n",
    "    for t in count():\n",
    "        action = agent.select_action(state)\n",
    "        action = map(action, -1, 1, min_action, max_action)\n",
    "        next_state, reward, done, info = env.step(np.float32(action))\n",
    "        ep_r += reward\n",
    "        #print(reward)\n",
    "        \n",
    "        # env.render()\n",
    "        if done: \n",
    "            # print(\"reward{}\".format(reward))\n",
    "            print(\"Episode \\t{}, the episode reward is \\t{:0.2f}\".format(i, ep_r))\n",
    "            ep_r = 0\n",
    "            # env.render()\n",
    "            break\n",
    "        state = next_state\n",
    "env.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
