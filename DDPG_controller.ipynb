{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Reinforcement Learning to Control the Rocket Hopper Demonstrator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Algorithm  - DDPG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion and Technical Details\n",
    "\n",
    "## Physical Simulation\n",
    "\n",
    "### Description\n",
    "\n",
    "The Rocket-Hopper Demonstrator only has 1 thruster that has no degree of gimballing, meaning that the only control parameter is the throttling of the engine. The thrust produced is a function of the mass flow that is allowed into the nozzle through the main valve. Therefore, the agent will have to control how open or closed the valve is.\n",
    "\n",
    "At the **start** of the simulation, the demonstrator is an altitude of *0 m* above the ground with an initial velocity of *0 m/s*. The objective is that the Rocket Hopper is capable of launching, staying at a specific altitude for a determined number of seconds, and then smoothly lands again.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- The atmosphere is neglected for the control of the agent. Even though the test campaign will occur within an enclosed building with no wind or gusts, there is still an aerodynamic force to be considered when \n",
    "\n",
    "- It is directed that the valve opens and closes instantaneously, when it is known that the valve has a characteristic opening and closing time for each. \n",
    "\n",
    "### Implementation\n",
    "\n",
    "The agent receives TBD observations at each timestep which are floating point values associated with the position, velocity and acceleration. \n",
    "\n",
    "The agent then acts and choses for each timestep, among TBD possible actions. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
