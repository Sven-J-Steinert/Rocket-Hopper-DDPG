{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Reinforcement Learning to Control the Rocket Hopper Demonstrator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Algorithm  - DDPG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion and Technical Details\n",
    "\n",
    "## Physical Simulation\n",
    "\n",
    "### Description\n",
    "\n",
    "The Rocket-Hopper Demonstrator only has 1 thruster that has no degree of gimballing, meaning that the only control parameter is the throttling of the engine. The thrust produced is a function of the mass flow that is allowed into the nozzle through the main valve. Therefore, the agent will have to control how open or closed the valve is.\n",
    "\n",
    "At the **start** of the simulation, the demonstrator is an altitude of *0 m* above the ground with an initial velocity of *0 m/s*. The objective is that the Rocket Hopper is capable of launching, staying at a specific altitude for a determined number of seconds, and then smoothly lands again.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- The atmosphere is neglected for the control of the agent. Even though the test campaign will occur within an enclosed building with no wind or gusts, there is still an aerodynamic force to be considered when \n",
    "\n",
    "- It is directed that the valve opens and closes instantaneously, when it is known that the valve has a characteristic opening and closing time for each. \n",
    "\n",
    "### Implementation\n",
    "\n",
    "The agent receives TBD observations at each timestep which are floating point values associated with the position, velocity and acceleration. \n",
    "\n",
    "The agent then acts and choses for each timestep, among TBD possible actions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries that are used in the Reinforcement Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch version: 2.1.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "from torch import nn                # needed for building neural networks\n",
    "import torch.nn.functional as F     # needed for activation functions\n",
    "import torch.optim as opt           # needed for optimisation\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import random\n",
    "from copy import copy, deepcopy\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "print(\"Using torch version: {}\".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Settings\n",
    "\n",
    "The configuration initially follows the [supplementary information section](https://arxiv.org/pdf/1509.02971.pdf?ref=blog.paperspace.com) of the DDPG paper in page 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE=1000000     # Buffer size of 1 million entries\n",
    "BATCH_SIZE=64   # Sampling from memory - This can be 128 for more complex tasks such as Hopper\n",
    "GAMMA=0.9\n",
    "TAU=0.001       #Target Network HyperParameters (soft update)\n",
    "LRA=0.0001      #LEARNING RATE - ACTOR\n",
    "LRC=0.001       #LEARNING RATE - CRITIC\n",
    "H1=400   #neurons of 1st layers\n",
    "H2=300   #neurons of 2nd layers\n",
    "\n",
    "MAX_EPISODES=50000  #number of episodes of the training\n",
    "MAX_STEPS=200       #max steps to finish an episode. An episode breaks early if some break conditions are met (like too much\n",
    "                    #amplitude of the joints angles or if a failure occurs)\n",
    "\n",
    "buffer_start = 100\n",
    "epsilon = 1\n",
    "epsilon_decay = 1./100000 #this is ok for a simple task like inverted pendulum, but maybe this would be set to zero for more\n",
    "                     #complex tasks like Hopper; epsilon is a decay for the exploration and noise applied to the action is \n",
    "                     #weighted by this decay. In more complex tasks we need the exploration to not vanish so we set the decay\n",
    "                     #to zero.\n",
    "PRINT_EVERY = 10 #Print info about average reward every PRINT_EVERY\n",
    "\n",
    "ENV_NAME = \"Pendulum-v1\" # For the hopper put \"Hopper-v2\" \n",
    "#check other environments to play with at https://gym.openai.com/envs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer\n",
    "Implement a simple replay buffer without priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class replayBuffer(object):\n",
    "    def __init__(self, buffer_size, name_buffer=''):\n",
    "        self.buffer_size=buffer_size  #choose buffer size\n",
    "        self.num_exp=0\n",
    "        self.buffer=deque()\n",
    "\n",
    "    def add(self, s, a, r, t, s2):\n",
    "        experience=(s, a, r, t, s2)\n",
    "        if self.num_exp < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.num_exp +=1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return self.buffer_size\n",
    "\n",
    "    def count(self):\n",
    "        return self.num_exp\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if self.num_exp < batch_size:\n",
    "            batch=random.sample(self.buffer, self.num_exp)\n",
    "        else:\n",
    "            batch=random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s, a, r, t, s2 = map(np.stack, zip(*batch))\n",
    "\n",
    "        return s, a, r, t, s2\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer = deque()\n",
    "        self.num_exp=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Setup \n",
    "\n",
    "For faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job will run on cpu\n"
     ]
    }
   ],
   "source": [
    "#set GPU for faster training\n",
    "cuda = torch.cuda.is_available() #check for CUDA\n",
    "device   = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(\"Job will run on {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture\n",
    "\n",
    "Network architecture is defined:\n",
    "\n",
    "- **Actor** is composed of:\n",
    "    - 3 FC (Fully-connected) Layers \n",
    "    - Hyperbolic tangent as the activation function -> Deals with a *-1,1* value range\n",
    "\n",
    "- **Critic** is composed of:\n",
    "    - Input: *state* & *action*\n",
    "    - Output: Q-value after 3 FC Layers\n",
    "\n",
    "\n",
    "In complex tasks, a *higher batch size* is required (128 instead of 64) and *batch normalisation layers* between input and hidden layers in both actor and critic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fanin_(size):\n",
    "    fan_in = size[0]\n",
    "    weight = 1./np.sqrt(fan_in)\n",
    "    return torch.Tensor(size).uniform_(-weight, weight)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, h1=H1, h2=H2, init_w=3e-3):\n",
    "        super(Critic, self).__init__()\n",
    "                \n",
    "        ## Neural Network of 3 Fully-Connected Layers and ReLU Activation Function\n",
    "        # FC1:       \n",
    "        self.linear1 = nn.Linear(state_dim, h1)\n",
    "        self.linear1.weight.data = fanin_(self.linear1.weight.data.size())\n",
    "        # Uncomment to use batch normalisation\n",
    "        #self.bn1 = nn.BatchNorm1d(h1)\n",
    "        # FC2:    (Action dimensions added to hidden layer dimensions)\n",
    "        self.linear2 = nn.Linear(h1+action_dim, h2)\n",
    "        self.linear2.weight.data = fanin_(self.linear2.weight.data.size())\n",
    "        # FC3  \n",
    "        self.linear3 = nn.Linear(h2, 1)\n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        # ReLU\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"\n",
    "        The state is passed through the neural network layers, as well as the action.\n",
    "        \"\"\"\n",
    "        x = self.linear1(state)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(torch.cat([x,action],1))\n",
    "        x = self.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class Actor(nn.Module): \n",
    "    def __init__(self, state_dim, action_dim, h1=H1, h2=H2, init_w=0.003):\n",
    "        super(Actor, self).__init__()        \n",
    "\n",
    "        ## Neural Network of 3 Fully-Connected Layers and TanH Activation Function\n",
    "        # Uncomment to use batch normalisation:\n",
    "        #self.bn0 = nn.BatchNorm1d(state_dim)\n",
    "        # FC1:\n",
    "        self.linear1 = nn.Linear(state_dim, h1)\n",
    "        self.linear1.weight.data = fanin_(self.linear1.weight.data.size())\n",
    "        # Uncomment to use batch normalisation:\n",
    "        #self.bn1 = nn.BatchNorm1d(h1)\n",
    "        # FC2:\n",
    "        self.linear2 = nn.Linear(h1, h2)\n",
    "        self.linear2.weight.data = fanin_(self.linear2.weight.data.size())\n",
    "        # Uncomment to use batch normalisation:\n",
    "        #self.bn2 = nn.BatchNorm1d(h2)\n",
    "        # FC3:        \n",
    "        self.linear3 = nn.Linear(h2, action_dim)\n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        # ReLU + Hyperbolic Tangent Activation Function:\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        The state is passed on through the neural network.\n",
    "        \"\"\"\n",
    "        # state = self.bn0(state)\n",
    "        x = self.linear1(state)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.tanh(x)\n",
    "        return x\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state  = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action = self.forward(state)\n",
    "        return action.detach().cpu().numpy()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration \n",
    "\n",
    "To ensure **exploration**, we have to add noise to the action. An **Ornstein-Uhlenbeck** process is chosen to add noise in a smooth way, suitable for continuous control tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu=0, sigma=0.2, theta=.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap state and action spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedEnv(gym.ActionWrapper):\n",
    "    \"\"\" Wrap action \"\"\"\n",
    "\n",
    "    def _action(self, action):\n",
    "        act_k = (self.action_space.high - self.action_space.low)/ 2.\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k * action + act_b\n",
    "\n",
    "    def _reverse_action(self, action):\n",
    "        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k_inv * (action - act_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training\n",
    "\n",
    "Initialize:\n",
    "- Environment\n",
    "- Networks \n",
    "- Target networks\n",
    "- Replay memory \n",
    "- Optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dim: 3, Action dim: 1\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(-1)       # So that results are reproducable, we always use the same seed\n",
    "\n",
    "# Initialize environment:\n",
    "env = NormalizedEnv(gym.make(ENV_NAME))\n",
    "\n",
    "# Define dimensions of state and action\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "print(\"State dim: {}, Action dim: {}\".format(state_dim, action_dim))\n",
    "\n",
    "# Add noise:\n",
    "noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\n",
    "\n",
    "# Call architectures:\n",
    "critic  = Critic(state_dim, action_dim).to(device)\n",
    "actor = Actor(state_dim, action_dim).to(device)\n",
    "\n",
    "target_critic  = Critic(state_dim, action_dim).to(device)\n",
    "target_actor = Actor(state_dim, action_dim).to(device)\n",
    "\n",
    "for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "\n",
    "for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "    \n",
    "\n",
    "# Initialize the optimizer\n",
    "q_optimizer  = opt.Adam(critic.parameters(),  lr=LRC)\n",
    "policy_optimizer = opt.Adam(actor.parameters(), lr=LRA)\n",
    "\n",
    "# Calculate the loss function\n",
    "MSE = nn.MSELoss()\n",
    "\n",
    "# Initialize replay memory \n",
    "memory = replayBuffer(BUFFER_SIZE)\n",
    "# writer = SummaryWriter() #initialise tensorboard writer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate Through Episodes\n",
    "\n",
    "MAX_EPISODES and MAX_STEPS parameters can be tuned according to the kind of the environment on which we are training the agent. \n",
    "\n",
    "In the case of a pendulum, we do not have a failure condition for each episode so it will always go through the max steps for each episode.\n",
    "\n",
    "In a task where there is a failure condition, an agent will not go through all the steps (at least at the beginning, when it has not yet learned how to accomplish the task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "def subplot(R, P, Q, S):\n",
    "    r = list(zip(*R))\n",
    "    p = list(zip(*P))\n",
    "    q = list(zip(*Q))\n",
    "    s = list(zip(*S))\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15,15))\n",
    "\n",
    "    ax[0, 0].plot(list(r[1]), list(r[0]), 'r') #row=0, col=0\n",
    "    ax[1, 0].plot(list(p[1]), list(p[0]), 'b') #row=1, col=0\n",
    "    ax[0, 1].plot(list(q[1]), list(q[0]), 'g') #row=0, col=1\n",
    "    ax[1, 1].plot(list(s[1]), list(s[0]), 'k') #row=1, col=1\n",
    "    ax[0, 0].title.set_text('Reward')\n",
    "    ax[1, 0].title.set_text('Policy loss')\n",
    "    ax[0, 1].title.set_text('Q loss')\n",
    "    ax[1, 1].title.set_text('Max steps')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 1 (got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Usuario\\Desktop\\PROJECTS\\Control-of-Rocket-Hopper\\DDPG_controller.ipynb Cell 23\u001b[0m line \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/PROJECTS/Control-of-Rocket-Hopper/DDPG_controller.ipynb#X25sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m epsilon \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m epsilon_decay\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/PROJECTS/Control-of-Rocket-Hopper/DDPG_controller.ipynb#X25sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m#actor.eval()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/PROJECTS/Control-of-Rocket-Hopper/DDPG_controller.ipynb#X25sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m a \u001b[39m=\u001b[39m actor\u001b[39m.\u001b[39;49mget_action(s)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/PROJECTS/Control-of-Rocket-Hopper/DDPG_controller.ipynb#X25sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m#actor.train()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/PROJECTS/Control-of-Rocket-Hopper/DDPG_controller.ipynb#X25sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m a \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m noise()\u001b[39m*\u001b[39m\u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, epsilon)\n",
      "\u001b[1;32mc:\\Users\\Usuario\\Desktop\\PROJECTS\\Control-of-Rocket-Hopper\\DDPG_controller.ipynb Cell 23\u001b[0m line \u001b[0;36mActor.get_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/PROJECTS/Control-of-Rocket-Hopper/DDPG_controller.ipynb#X25sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_action\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/PROJECTS/Control-of-Rocket-Hopper/DDPG_controller.ipynb#X25sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     state  \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mFloatTensor(state)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/PROJECTS/Control-of-Rocket-Hopper/DDPG_controller.ipynb#X25sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(state)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/PROJECTS/Control-of-Rocket-Hopper/DDPG_controller.ipynb#X25sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m action\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 3 at dim 1 (got 0)"
     ]
    }
   ],
   "source": [
    "plot_reward = []\n",
    "plot_policy = []\n",
    "plot_q = []\n",
    "plot_steps = []\n",
    "\n",
    "\n",
    "best_reward = -np.inf\n",
    "saved_reward = -np.inf\n",
    "saved_ep = 0\n",
    "average_reward = 0\n",
    "global_step = 0\n",
    "#s = deepcopy(env.reset())\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    #print(episode)\n",
    "    s = deepcopy(env.reset())\n",
    "    #noise.reset()\n",
    "\n",
    "    ep_reward = 0.\n",
    "    ep_q_value = 0.\n",
    "    step=0\n",
    "\n",
    "    for step in range(MAX_STEPS):\n",
    "        #loss=0\n",
    "        global_step +=1\n",
    "        epsilon -= epsilon_decay\n",
    "        #actor.eval()\n",
    "        a = actor.get_action(s)\n",
    "        #actor.train()\n",
    "\n",
    "        a += noise()*max(0, epsilon)\n",
    "        a = np.clip(a, -1., 1.)\n",
    "        s2, reward, terminal, info = env.step(a)\n",
    "\n",
    "\n",
    "        memory.add(s, a, reward, terminal,s2)\n",
    "\n",
    "        #keep adding experiences to the memory until there are at least minibatch size samples\n",
    "        \n",
    "        if memory.count() > buffer_start:\n",
    "            s_batch, a_batch, r_batch, t_batch, s2_batch = memory.sample(BATCH_SIZE)\n",
    "\n",
    "            s_batch = torch.FloatTensor(s_batch).to(device)\n",
    "            a_batch = torch.FloatTensor(a_batch).to(device)\n",
    "            r_batch = torch.FloatTensor(r_batch).unsqueeze(1).to(device)\n",
    "            t_batch = torch.FloatTensor(np.float32(t_batch)).unsqueeze(1).to(device)\n",
    "            s2_batch = torch.FloatTensor(s2_batch).to(device)\n",
    "            \n",
    "            \n",
    "            #compute loss for critic\n",
    "            a2_batch = target_actor(s2_batch)\n",
    "            target_q = target_critic(s2_batch, a2_batch)\n",
    "            y = r_batch + (1.0 - t_batch) * GAMMA * target_q.detach() #detach to avoid backprop target\n",
    "            q = critic(s_batch, a_batch)\n",
    "            \n",
    "            q_optimizer.zero_grad()\n",
    "            q_loss = MSE(q, y) \n",
    "            q_loss.backward()\n",
    "            q_optimizer.step()\n",
    "            \n",
    "            #compute loss for actor\n",
    "            policy_optimizer.zero_grad()\n",
    "            policy_loss = -critic(s_batch, actor(s_batch))\n",
    "            policy_loss = policy_loss.mean()\n",
    "            policy_loss.backward()\n",
    "            policy_optimizer.step()\n",
    "            \n",
    "            #soft update of the frozen target networks\n",
    "            for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - TAU) + param.data * TAU\n",
    "                )\n",
    "\n",
    "            for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - TAU) + param.data * TAU\n",
    "                )\n",
    "\n",
    "        s = deepcopy(s2)\n",
    "        ep_reward += reward\n",
    "\n",
    "\n",
    "        #if terminal:\n",
    "        #    noise.reset()\n",
    "        #    break\n",
    "\n",
    "    try:\n",
    "        plot_reward.append([ep_reward, episode+1])\n",
    "        plot_policy.append([policy_loss.data, episode+1])\n",
    "        plot_q.append([q_loss.data, episode+1])\n",
    "        plot_steps.append([step+1, episode+1])\n",
    "    except:\n",
    "        continue\n",
    "    average_reward += ep_reward\n",
    "    \n",
    "    if ep_reward > best_reward:\n",
    "        torch.save(actor.state_dict(), 'best_model_pendulum.pkl') #Save the actor model for future testing\n",
    "        best_reward = ep_reward\n",
    "        saved_reward = ep_reward\n",
    "        saved_ep = episode+1\n",
    "\n",
    "    if (episode % PRINT_EVERY) == (PRINT_EVERY-1):    # print every print_every episodes\n",
    "        subplot(plot_reward, plot_policy, plot_q, plot_steps)\n",
    "        print('[%6d episode, %8d total steps] average reward for past {} iterations: %.3f'.format(PRINT_EVERY) %\n",
    "              (episode + 1, global_step, average_reward / PRINT_EVERY))\n",
    "        print(\"Last model saved with reward: {:.2f}, at episode {}.\".format(saved_reward, saved_ep))\n",
    "        average_reward = 0 #reset average reward\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
