{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875ecbac-1b96-4d6f-b8e9-b552108823c2",
   "metadata": {},
   "source": [
    "from this article\n",
    "\n",
    "https://webcache.googleusercontent.com/search?q=cache:https://arshren.medium.com/step-by-step-guide-to-implementing-ddpg-reinforcement-learning-in-pytorch-9732f42faac9&sca_esv=589611474&strip=1&vwsrc=0\n",
    "\n",
    "had to be highly adapted to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72a5e764-3195-4d64-83e8-908fe071573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from itertools import count\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6aca6c3e-10ec-4e88-9e77-39a0149bda46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay_buffer():\n",
    "    '''\n",
    "    Code based on:\n",
    "    https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "    Expects tuples of (state, next_state, action, reward, done)\n",
    "    '''\n",
    "    def __init__(self, max_size=100):\n",
    "        \"\"\"Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        \"\"\"\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def push(self, data):\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = data\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        state: np.array\n",
    "            batch of state or observations\n",
    "        action: np.array\n",
    "            batch of actions executed given a state\n",
    "        reward: np.array\n",
    "            rewards received as results of executing action\n",
    "        next_state: np.array\n",
    "            next state next state or observations seen after executing action\n",
    "        done: np.array\n",
    "            done[i] = 1 if executing ation[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        state, next_state, action, reward, done = [], [], [], [], []\n",
    "\n",
    "        for i in ind:\n",
    "            st, n_st, act, rew, dn = self.storage[i]\n",
    "            state.append(np.array(st, copy=False))\n",
    "            next_state.append(np.array(n_st, copy=False))\n",
    "            action.append(np.array(act, copy=False))\n",
    "            reward.append(np.array(rew, copy=False))\n",
    "            done.append(np.array(dn, copy=False))\n",
    "\n",
    "        return np.array(state), np.array(next_state), np.array(action), np.array(reward).reshape(-1, 1), np.array(done).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b9fc2f6-ba88-48ca-b01c-0c1fe07cfad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"\n",
    "    The Actor model takes in a state observation as input and \n",
    "    outputs an action, which is a continuous value.\n",
    "    \n",
    "    It consists of four fully connected linear layers with ReLU activation functions and \n",
    "    a final output layer selects one single optimized action for the state\n",
    "    \"\"\"\n",
    "    def __init__(self, n_states, action_dim, hidden1):\n",
    "        super(Actor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_states, hidden1), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden1, hidden1), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden1, hidden1), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden1, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"\n",
    "    The Critic model takes in both a state observation and an action as input and \n",
    "    outputs a Q-value, which estimates the expected total reward for the current state-action pair. \n",
    "    \n",
    "    It consists of four linear layers with ReLU activation functions, \n",
    "    State and action inputs are concatenated before being fed into the first linear layer. \n",
    "    \n",
    "    The output layer has a single output, representing the Q-value\n",
    "    \"\"\"\n",
    "    def __init__(self, n_states, action_dim, hidden2):\n",
    "        super(Critic, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(int(n_states) + int(action_dim), int(hidden2)), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(int(hidden2), int(hidden2)), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(int(hidden2), int(hidden2)), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(int(hidden2), int(action_dim))\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        return self.net(torch.cat((state, action), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "79f6cd49-e943-46a1-8bb3-32d50c88bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OU_Noise(object):\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\n",
    "    code from :\n",
    "    https://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "    The OU_Noise class has four attributes\n",
    "    \n",
    "        size: the size of the noise vector to be generated\n",
    "        mu: the mean of the noise, set to 0 by default\n",
    "        theta: the rate of mean reversion, controlling how quickly the noise returns to the mean\n",
    "        sigma: the volatility of the noise, controlling the magnitude of fluctuations\n",
    "    \"\"\"\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\n",
    "        This method uses the current state of the noise and generates the next sample\n",
    "        \"\"\"\n",
    "        dx = self.theta * (self.mu - self.state) + self.sigma * np.array([np.random.normal() for _ in range(len(self.state))])\n",
    "        self.state += dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3065c9fe-88c5-4c7e-adda-ca6ab93e526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Hyperparameters\n",
    "# Hyperparameters adapted for performance from\n",
    "#https://ai.stackexchange.com/questions/22945/ddpg-doesnt-converge-for-mountaincarcontinuous-v0-gym-environment\n",
    "capacity=1000000\n",
    "batch_size=64\n",
    "update_iteration=200\n",
    "tau=0.001 # tau for soft updating\n",
    "gamma=0.99 # discount factor\n",
    "directory = './'\n",
    "hidden1=20 # hidden layer for actor\n",
    "hidden2=64. #hiiden laye for critic\n",
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        \"\"\"\n",
    "        Initializes the DDPG agent. \n",
    "        Takes three arguments:\n",
    "               state_dim which is the dimensionality of the state space, \n",
    "               action_dim which is the dimensionality of the action space, and \n",
    "               max_action which is the maximum value an action can take. \n",
    "        \n",
    "        Creates a replay buffer, an actor-critic  networks and their corresponding target networks. \n",
    "        It also initializes the optimizer for both actor and critic networks alog with \n",
    "        counters to track the number of training iterations.\n",
    "        \"\"\"\n",
    "        self.replay_buffer = Replay_buffer()\n",
    "        \n",
    "        self.actor = Actor(state_dim, action_dim, hidden1).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim,  hidden1).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-3)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim,  hidden2).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim,  hidden2).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=2e-2)\n",
    "        # learning rate\n",
    "\n",
    "        \n",
    "\n",
    "        self.num_critic_update_iteration = 0\n",
    "        self.num_actor_update_iteration = 0\n",
    "        self.num_training = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Takes the current state as input and returns an action to take in that state.\n",
    "        It uses the actor network to map the state to an action.\n",
    "        \"\"\"\n",
    "        if isinstance(state, tuple) and len(state) == 2:\n",
    "            # Assuming state is a tuple with two elements\n",
    "            state_part1 = np.array(state[0], dtype=float).flatten()\n",
    "            state_part2 = self.convert_dict_to_array(state[1]).flatten()\n",
    "            state_array = np.concatenate([state_part1, state_part2]).reshape(1, -1)\n",
    "        else:\n",
    "            # Fallback to a default behavior (you might need to adjust this based on your actual state structure)\n",
    "            state_array = np.array(state, dtype=float).reshape(1, -1)\n",
    "\n",
    "        state_tensor = torch.FloatTensor(state_array).to(device)\n",
    "        return self.actor(state_tensor).cpu().data.numpy().flatten()\n",
    "\n",
    "    def convert_dict_to_array(self, state_dict):\n",
    "        \"\"\"\n",
    "        Converts a dictionary to a 1D NumPy array.\n",
    "        \"\"\"\n",
    "        state_values = np.array(list(state_dict.values()), dtype=float)\n",
    "        return state_values\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        updates the actor and critic networks using a batch of samples from the replay buffer. \n",
    "        For each sample in the batch, it computes the target Q value using the target critic network and the target actor network. \n",
    "        It then computes the current Q value \n",
    "        using the critic network and the action taken by the actor network. \n",
    "        \n",
    "        It computes the critic loss as the mean squared error between the target Q value and the current Q value, and \n",
    "        updates the critic network using gradient descent. \n",
    "        \n",
    "        It then computes the actor loss as the negative mean Q value using the critic network and the actor network, and \n",
    "        updates the actor network using gradient ascent. \n",
    "        \n",
    "        Finally, it updates the target networks using \n",
    "        soft updates, where a small fraction of the actor and critic network weights are transferred to their target counterparts. \n",
    "        This process is repeated for a fixed number of iterations.\n",
    "        \"\"\"\n",
    "\n",
    "        for it in range(update_iteration):\n",
    "            # For each Sample in replay buffer batch\n",
    "            state, next_state, action, reward, done = self.replay_buffer.sample(batch_size)\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            action = torch.FloatTensor(action).to(device)\n",
    "            next_state = torch.FloatTensor(next_state).to(device)\n",
    "            done = torch.FloatTensor(1-done).to(device)\n",
    "            reward = torch.FloatTensor(reward).to(device)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            target_Q = self.critic_target(next_state, self.actor_target(next_state))\n",
    "            target_Q = reward + (done * gamma * target_Q).detach()\n",
    "\n",
    "            # Get current Q estimate\n",
    "            current_Q = self.critic(state, action)\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = F.mse_loss(current_Q, target_Q)\n",
    "            \n",
    "            # Optimize the critic\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # Compute actor loss as the negative mean Q value using the critic network and the actor network\n",
    "            actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "\n",
    "            # Optimize the actor\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Update the frozen target models using \n",
    "            soft updates, where \n",
    "            tau,a small fraction of the actor and critic network weights are transferred to their target counterparts. \n",
    "            \"\"\"\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            \n",
    "           \n",
    "            self.num_actor_update_iteration += 1\n",
    "            self.num_critic_update_iteration += 1\n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        Saves the state dictionaries of the actor and critic networks to files\n",
    "        \"\"\"\n",
    "        torch.save(self.actor.state_dict(), directory + 'actor.pth')\n",
    "        torch.save(self.critic.state_dict(), directory + 'critic.pth')\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Loads the state dictionaries of the actor and critic networks to files\n",
    "        \"\"\"\n",
    "        self.actor.load_state_dict(torch.load(directory + 'actor.pth'))\n",
    "        self.critic.load_state_dict(torch.load(directory + 'critic.pth')) # Train the agent on a Mountain car using DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03f4765b-057c-49e4-8eaa-c9482c6a65b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the environment\n",
    "env_name='MountainCarContinuous-v0'\n",
    "env = gym.make(env_name)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define different parameters for training the agent\n",
    "max_episode=100\n",
    "max_time_steps=5000\n",
    "ep_r = 0\n",
    "total_step = 0\n",
    "score_hist=[]\n",
    "# for rensering the environmnet\n",
    "render=True\n",
    "render_interval=10\n",
    "# for reproducibility\n",
    "#env.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "#Environment action ans states\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "min_Val = torch.tensor(1e-7).float().to(device) \n",
    "\n",
    "# Exploration Noise\n",
    "exploration_noise=0.1\n",
    "exploration_noise=0.1 * max_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5da8913a-cd9b-44f6-abba-626f5e89c24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -0.0020285772190065113\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m ep_r \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset episode reward for each iteration\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m count():\n\u001b[1;32m---> 10\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(np\u001b[38;5;241m.\u001b[39mfloat32(action))\n\u001b[0;32m     12\u001b[0m     ep_r \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[55], line 60\u001b[0m, in \u001b[0;36mDDPG.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     57\u001b[0m     state_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(state, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     59\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state_array)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Assuming you have a DDPG class defined somewhere in your code\n",
    "agent = DDPG(state_dim, action_dim)\n",
    "\n",
    "test_iteration = 100\n",
    "\n",
    "for i in range(test_iteration):\n",
    "    state = env.reset()\n",
    "    ep_r = 0  # Reset episode reward for each iteration\n",
    "    for t in count():\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, _, _ = env.step(np.float32(action))\n",
    "        ep_r += reward\n",
    "        if t % 1000 == 0:\n",
    "            print(f'{i} {reward}',end='\\r',flush=True)\n",
    "        env.render()\n",
    "        if done:\n",
    "            print(\"reward{}\".format(reward))\n",
    "            print(\"Episode \\t{}, the episode reward is \\t{:0.2f}\".format(i, ep_r))\n",
    "            env.render()\n",
    "            break\n",
    "        state = next_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab76526-60db-4b95-bdeb-702a58ef4369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e2b58-845c-4b00-8725-7861a8d3b274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
